---
output:
  pdf_document:
    fig_caption: yes
    number_sections: no
    toc: yes
    toc_depth: 2
    # github_document:
    # html_preview: yes
    # toc: yes
    # toc_depth: 2
---

```{r setup, include=FALSE, warning=FALSE, message=FALSE}

knitr::opts_chunk$set(echo=FALSE, message=FALSE, warning=FALSE)

library(ggplot2)
library(dplyr)
library(tibble)
library(cowplot)
library(beepr)
library(Rfast)
library(changepoint)
library(patchwork)
library(mcp)
library(rjags)
source("./code/functions.R")
source("./code/theme_gar.txt")
# Edit `one_over_f` function from `primer` package to control variance (Stevens, 2009). 
# Original function is available on [GitHub](https://github.com/HankStevens/primer).
# Copyright Hank Stevens.
source("./code/one_over_f.R")
# Load template: true onset = 160 ms, F=81, max at F=126
source("./code/erp_template.R")
# R version of Matlab code from Yeung et al. 2004
source("./code/eeg_noise.R")
# Lodaing the code that gives median instead of mean values in mcp summary table
source("./code/mcp_median_function.R")
# to use with eeg_noise function
meanpower <- unlist(read.table("./code/meanpower.txt"))
```

## Bayesian Multiple Change Points (mcp) Detection Algorithm 
### Example time course

Bayesian change point algorithm mcp (Lindelov, 2020) was applied to an example EEG time course from one participant. Uniform prior of 100-200ms was chosen. Figure shows that the algorithm estimated effect onset at 178ms (true onset being at 160ms).

```{r}

options(mc.cores = 3) # speed up sampling
set.seed(666)
srate <- 500 # Sampling rate in Hz
Nt <- 50 # number of trials
outvar <- 1 # noise variance
cond1 <- matrix(0, nrow = Nt, ncol = Nf) 
cond2 <- matrix(0, nrow = Nt, ncol = Nf)

for(T in 1:Nt){
  cond2[T,] <- temp2 + eeg_noise(frames = Nf, srate = srate, outvar = outvar, meanpower)
  cond1[T,] <- temp1 + eeg_noise(frames = Nf, srate = srate, outvar = outvar, meanpower)
}

ori.t2 <- vector(mode = "numeric", length = Nf)

for(F in 1:Nf){
  ori.t2[F] <- t.test(cond1[,F], cond2[,F])$statistic^2
}

df <- tibble(x = Xf,
             y = ori.t2)

# results <- list()

# for (i in 1:2){
# Model
model <- list(
       y ~ 1 + sigma(1),          # Before change point: constant mean level
         ~ 0 + x + sigma(1)      # After change point: different mean level (no intercept)
)

prior <- list(
  cp_1 = "dunif(100, 200)" # change point expected between 100 and 200 ms
)

fit <- mcp(model, data = df, prior = prior, chains = 3, cores = 3)

# results[[i]] <- fit

# }

summary_fit <- summary.mcpfit.median(fit)
summary.mcpfit.median(fit)
# table.1 <- tibble(table.1)

# Plotting
p.mcp <- ggplot(df, aes(x, y)) + theme_gar + 
  geom_line(linewidth = 1) +
  geom_vline(xintercept = true_onset) +
  geom_vline(xintercept = summary_fit[1,2], linetype = "dotted") +
  labs(x = "Time in ms", y = bquote(t^2)) +
  ggtitle(paste("Change point onset =", round(summary_fit[1,2], 0), "ms"))
p.mcp
mcp:::summary.mcpfit(fit)

# Assuming 'fit' is your mcp model fit object
summary_fit <- summary.mcpfit.median(fit)

# Convert the summary to a data frame if it's not already one
summary_df <- as.data.frame(summary_fit)

# Apply rounding to all numeric columns in the data frame
summary_df[] <- lapply(summary_df, function(x) if(is.numeric(x)) round(x, 2) else x)

# View the rounded summary table
print(summary_df)
```

```{r fig.height=4, fig.width=5, fig.cap="Example time course showing the change point location estimated by Bayesian mcp."}
p.mcp
```

```{r fig.cap="Prior predictive fit of the raw data (black dots), 25 draws of the posterior (gray lines), 95% high density interval (dashed red lines) and posterior distribution of the change point (blue line)."}
plot(fit, q_fit = TRUE) + ggplot2::xlab("Time in ms") + ggplot2::ylab(bquote(t^2)) + theme_gar
```


```{r include=FALSE}
par(mf.row = c(1,1))

fig1 <- plot(fit)
fig2 <- pp_check(fit)

fig <- fig1 + fig2
```


### Posterior distribution plots

Below is a collection of plots showing the posterior distributions of the model that was fitted to the example time course in figure 1. cp_1 density plot shows the posterior distribution of the change point. The peak of the curve shows the most likely location of the change point and the spread of the curve indicates the uncertainty around the estimate. The trace plot shows a lot of spikes indicating lack of consistency and convergence. However, Rhat values (table 1), a measure used to assess the convergence of MCMC simulations, are less than 1.1 meaning the variance between chains is similar to the variance within chains which suggests the chains are converged. This likely means that the chains are exploring the full posterior distribution as well as showing there are no obvious issues with the model.  

`r table.1`

int_1 density plot shows the posterior distribution of the intercept (predicted value of the dependent variable when all predictors in the model are set to 0). It shows the distribution of the data before the change point (segment 1). 

x_2 is the distribution of the slope parameter for segment 2 that takes place after the change point. 

sigma_1 and sigma_2 plots represent the estimated range and distribution of the standard deviation of the model's errors and residuals in the first and second segments. The higher values of the second segment mean that the second segment is a lot more volatile or less predictable than the first segment.

The units for the axis labels are in ms for the distribution plots, with the segment plots (int_1 and x_2) and the residual plots (sigma_1 and sigma_2) x-axis showing the standard deviation in ms. 

The mcp algorithm is designed so that the calculated change point is a mean value of the posterior distribution calculated using the hdci_mean function from the tidybayes package (tidybayes citation). To minimise the effect of outliers, the algorithm was edited to use hdci_median function instead to provide the change point location as the median of the posterior distribution instead. The edited function can be found in (github link to the code folder where mcp_median_function is). 

```{r fig.height=8, fig.width=8, fig.cap="int_1 density plot shows the posterior distribution of the intercept (predicted value of the dependent variable when all predictors in the model are set to 0). It shows the distribution of the data before the change point (segment 1). x_2 is the distribution of the slope parameter for segment 2 that takes place after the change point. sigma_1 and sigma_2 plots represent the estimated range and distribution of the standard deviation of the model's errors and residuals in the first and second segments. The higher values of the second segment mean that the second segment is a lot more volatile or less predictable than the first segment. The units for the axis labels are in ms for the distribution plots, with the segment plots (int_1 and x_2) and the residual plots (sigma_1 and sigma_2) x-axis showing the standard deviation in ms."}
plot_pars(fit) # This command is broken in the CRAN version of mcp but works if you install mcp development version from github
```


### Prior sensitivity analysis

To determine the extent to which the prior is influencing the results instead of the data, the model was also tested with a default prior (figure x), a narrow prior of 150-180ms (figure x) and a wide prior of 50-300ms (figure x). 

```{r message=FALSE, fig.height=8, fig.width=8}
options(mc.cores = 3)
# set.seed(666)
srate <- 500 # Sampling rate in Hz
Nt <- 50 # number of trials
outvar <- 1 # noise variance
cond1 <- matrix(0, nrow = Nt, ncol = Nf) 
cond2 <- matrix(0, nrow = Nt, ncol = Nf)

for(T in 1:Nt){
  cond2[T,] <- temp2 + eeg_noise(frames = Nf, srate = srate, outvar = outvar, meanpower)
  cond1[T,] <- temp1 + eeg_noise(frames = Nf, srate = srate, outvar = outvar, meanpower)
}

ori.t2 <- vector(mode = "numeric", length = Nf)

for(F in 1:Nf){
  ori.t2[F] <- t.test(cond1[,F], cond2[,F])$statistic^2
}

df <- tibble(x = Xf,
             y = ori.t2)

model <- list(
       y ~ 1 + sigma(1),          # Before change point: constant mean level
         ~ 0 + x + sigma(1)      # After change point: different mean level (no intercept)
)

fit_default <- mcp(model, data = df, cores = 3, chains = 3)

# summary.mcpfit.median(fit_default)
plot(fit_default, q_fit = TRUE) + 
ggplot2::xlab("Time in ms") + ggplot2::ylab(bquote(t^2)) + theme_gar

summary_fit_default <- summary.mcpfit.median(fit_default)


# Plotting
# p.mcp <- ggplot(df, aes(x, y)) + theme_gar +
#   geom_line(linewidth = 1) +
#   geom_vline(xintercept = true_onset) +
#   geom_vline(xintercept = summary_fit_default[1,2], linetype = "dotted") +
#   labs(x = "Time in ms", y = bquote(t^2)) +
#   ggtitle(paste("Change point onset =", round(summary_fit_default[1,2],0), "ms"))

pp_check(fit_default)
plot_pars(fit_default)

# fig.1 <- fig1 / fig2


```

```{r fig.cap="Upper panel shows the fit of the data (black points) and the 25 posterior draws (gray lines). There are two posterior distributions visible (blue lines) indicating the location of the effect onset and offset. Lower panel is a plot of the posterior predictive check showing that the model generally captures the trend of the data. Blue lines show the posterior predictive and black line shows the observed data."}
pp_check(fit_default)
```
```{r fig.cap="figure showing plots"}
fig.2
```

When no prior is set, the algorithm naturally detects two change points in the data, one around the time of the true onset and the second one around the time of the offset. This is reflected in the trace plots where there are two traces for different change points that are detected. 

```{r eval=FALSE, fig.cap="Narrow prior"}

options(mc.cores = 3)

results_narrow <- list()

for (i in 1:1){
prior_narrow <- list(
  cp_1 = "dunif(150, 180)" # change point expected between 150 and 180 ms
)


fit_narrow <- mcp(model, data = df, prior = prior_narrow, cores = 3, chains = 3)

results_narrow[[i]] <- fit_narrow

}

summary(fit_narrow)
plot(fit_narrow, q_fit = TRUE) + ggplot2::xlab("Time in ms") + ggplot2::ylab(bquote(t^2)) + theme_gar
summary_fit_narrow <- summary(fit_narrow)


# Plotting
p.mcp <- ggplot(df, aes(x, y)) + theme_gar +
  geom_line(linewidth = 1) +
  geom_vline(xintercept = true_onset) +
  geom_vline(xintercept = summary_fit_narrow[1,2], linetype = "dotted") +
  labs(x = "Time in ms", y = bquote(t^2)) +
  ggtitle(paste("Change point onset =", round(summary_fit_narrow[1,2],0), "ms"))
p.mcp

pp_check(fit_narrow)
plot_pars(fit_narrow)
```

```{r fig.height=8, fig.width=8}
plot_pars(fit_narrow)
```

```{r eval=FALSE, fig.cap="Wide prior"}

options(mc.cores = 3)

results_wide <- list()

for (i in 1:1){
prior_wide <- list(
  cp_1 = "dunif(50, 300)" # change point expected between 150 and 180 ms
)


fit_wide <- mcp(model, data = df, prior = prior_wide, cores = 3, chains = 3)

results_wide[[i]] <- fit_wide

}

summary(fit_wide)
plot(fit_wide, q_fit = TRUE) + ggplot2::xlab("Time in ms") + ggplot2::ylab(bquote(t^2)) + theme_gar
summary_fit_wide <- summary(fit_wide)


# Plotting
p.mcp <- ggplot(df, aes(x, y)) + theme_gar +
  geom_line(linewidth = 1) +
  geom_vline(xintercept = true_onset) +
  geom_vline(xintercept = summary_fit_wide[1,2], linetype = "dotted") +
  labs(x = "Time in ms", y = bquote(t^2)) +
  ggtitle(paste("Change point onset =", round(summary_fit_wide[1,2],0), "ms"))
p.mcp

pp_check(fit_wide)
```

```{r fig.height=8, fig.width=8}
plot_pars(fit_wide)
```


Looking over the above figures, it appears that the prior only has a limited impact on where the change point is detected, with the default prior showing the location of the offset also. Therefore, a uniform prior of 100-200 ms is considered appropriate for this study to allow for the Bayesian change point detection method to be compared to other methods without the need to compare different priors. Additionally, the default prior was not chosen as it detects the location of effect onset which is not the focus of this project. 


## Pruned Exact Linear Time (PELT) approach to change point detection
### Penalty multiplier 
-introduce pelt-
By design, PELT automatically identifies all change points that it can detect in the data, with no option for the user to set how many change points they are seeking to detect unlike in the implementation of the Binary Segmentation algorithm. However, it is still possible to change the penalty multiplier setting which controls the trade off between fitting the data closely and identifying many change points versus keeping the model simpler and having fewer change points. The formula for calculating the penalty value is k \*\ log(n) where k is the penalty multiplier and n is the number of observations in the time series. This provides a logarithmic scale the penalty which grows with the data and not linearly. In this project, the penalty multiplier value of 15 was determined to be most optimal after a comparison against all other multiplier values between 1-30. The comparison was conducted by running 1000 simulations at every multiplier from 1-30 and comparing the mean, median and standard deviation of change point estimates. The results are shown in figure x. 

```{r eval=FALSE}

srate <- 500 # Sampling rate in Hz
nsim <- 1000
Nt <- 50 # number of trials
outvar <- 2 # noise variance
cond1 <- matrix(0, nrow = Nt, ncol = Nf) 
cond2 <- matrix(0, nrow = Nt, ncol = Nf)

results <- data.frame(multiplier = integer(), mean = numeric(), sd = numeric(), median = numeric())

for (multiplier in 0:30) {
    pt.res <- list()

    for (i in 1:nsim) {
      
      for(T in 1:Nt){
        cond2[T,] <- temp2 + eeg_noise(frames = Nf, srate = srate, outvar = outvar, meanpower)
        cond1[T,] <- temp1 + eeg_noise(frames = Nf, srate = srate, outvar = outvar, meanpower)
      }
      
      ori.t2 <- vector(mode = "numeric", length = Nf)
      
      for(F in 1:Nf){
        ori.t2[F] <- t.test(cond1[,F], cond2[,F])$statistic^2
      }
      
      # Adjust penalty based on the multiplier
      pts <- cpt.meanvar(ori.t2, method = "PELT", penalty = "Manual", pen.value = multiplier * log(length(ori.t2)))
      if (length(pts@cpts) > 0) {
        pt.res[[i]] <- Xf[pts@cpts[1]]
        } else {
          pt.res[[i]] <- NA  # When no change point is found
        }
      }

    # Calculate statistics for this multiplier
    current_mean <- mean(unlist(pt.res), na.rm = TRUE)
    current_sd <- sd(unlist(pt.res), na.rm = TRUE)
    current_median <- median(unlist(pt.res), na.rm = TRUE)

    # Store results
    results <- rbind(results, data.frame(multiplier = multiplier, 
                                         mean = current_mean, 
                                         sd = current_sd, 
                                         median = current_median))
}

save(results, file = "./data/pen_multiplier_sim_appendix.RData")
```
```{r}
load("./data/pen_multiplier_sim_appendix.RData")
ggplot(data = results, aes(x = multiplier)) +
  geom_line(aes(y = mean, color = "Mean"), size = 1) +
  geom_point(aes(y = mean, color = "Mean"), size = 2) +
  geom_line(aes(y = sd, color = "SD"), size = 1) +
  geom_point(aes(y = sd, color = "SD"), size = 2) +
  geom_line(aes(y = median, color = "Median"), size = 1) +
  geom_point(aes(y = median, color = "Median"), size = 2) +
  scale_color_manual(values = c("Mean" = "dodgerblue", "SD" = "red4", "Median" = "green4")) +
  labs(title = "Change Point Analysis: Mean, SD, and Median",
       x = "Log Multiplier", y = "Value") +
  theme_minimal() +
  theme(legend.title = element_blank(), legend.position = "top")
```

Above figure shows that multiplier values below 10 lead to the majority of change points to be detected too early, as indicated by lower median change point values. Meanwhile, multiplier values above 20 lead to better onset estimates (given the real onset is set for 160ms), however, there is greater variability in the onsets estimated with these multipliers as indicated by the higher standard deviations. Therefore, a log multiplier value of 15 was chosen, though it could be argued that other penalty multiplier values between 10 and 20 would also be suitable.

Applying the logarithmic penalty with a multiplier of 15 on an example time course leads to an estimated effect onset of 186ms. However, since the penalty limits the sensitivity of the algorithm and not the number of change points that can be detected, the algorithm still picks up the the time location of the signal offset. 

```{r}
set.seed(666)
srate <- 500 # Sampling rate in Hz
Nt <- 50 # number of trials
outvar <- 1 # noise variance
cond1 <- matrix(0, nrow = Nt, ncol = Nf) 
cond2 <- matrix(0, nrow = Nt, ncol = Nf)

for(T in 1:Nt){
  cond2[T,] <- temp2 + eeg_noise(frames = Nf, srate = srate, outvar = outvar, meanpower)
  cond1[T,] <- temp1 + eeg_noise(frames = Nf, srate = srate, outvar = outvar, meanpower)
}

ori.t2 <- vector(mode = "numeric", length = Nf)

for(F in 1:Nf){
  ori.t2[F] <- t.test(cond1[,F], cond2[,F])$statistic^2
}


res <- cpt.meanvar(ori.t2, method = "PELT", penalty = "Manual", pen.value = 15*log(length(ori.t2)))

df <- tibble(x = Xf,
             y = ori.t2)

p.cp <- ggplot(df, aes(x, y)) + theme_gar + 
  geom_line(linewidth = 1) +
  geom_vline(xintercept = true_onset) +
  geom_vline(xintercept = Xf[res@cpts[1]], linetype = "dotted") +
  labs(x = "Time in ms", y = bquote(t^2)) +
  ggtitle(paste("Change point onset =", Xf[res@cpts[1]], "ms"))
p.cp

plot(res, cpt.width = 4)
# cpts(res)
```

The penalty multiplier appears to be an effective method of limiting the sensitivity of the PELT algorithm when compared to running the algorithm without a penalty setting. 

```{r, fig.cap="Different change points detected in the example time course by the PELT algorithm when no penalty is applied. Red lines correspond to separate segments with differing mean and variance by the PELT algorithm."}
set.seed(666)
srate <- 500 # Sampling rate in Hz
Nt <- 50 # number of trials
outvar <- 1 # noise variance
cond1 <- matrix(0, nrow = Nt, ncol = Nf) 
cond2 <- matrix(0, nrow = Nt, ncol = Nf)

for(T in 1:Nt){
  cond2[T,] <- temp2 + eeg_noise(frames = Nf, srate = srate, outvar = outvar, meanpower)
  cond1[T,] <- temp1 + eeg_noise(frames = Nf, srate = srate, outvar = outvar, meanpower)
}

ori.t2 <- vector(mode = "numeric", length = Nf)

for(F in 1:Nf){
  ori.t2[F] <- t.test(cond1[,F], cond2[,F])$statistic^2
}


res <- cpt.meanvar(ori.t2, method = "PELT")

plot(res, cpt.width = 4)
```

### Onset distributions 
To understand how varying penalty multiplier values can affect the distribution of onsets across changing EEG data, a simulation was done with 10,000 iterations for every penalty multiplier value. The results, shown below, indicate that a lot of onsets are being severely underestimated, as evidenced by the big left tail. In particular, the tail is heavily populated by onsets obtained from using lower penalty values (identified by purple shaded lines). 

```{r warning=FALSE, fig.height=10, fig.width=10}

load(file = "./data/simres_list_penalty_comparison.RData")

df <- tibble(
  onsets = c(simres_list$simres.cp1,
             simres_list$simres.cp2,
             simres_list$simres.cp3,
             simres_list$simres.cp4,
             simres_list$simres.cp5,
             simres_list$simres.cp6,
             simres_list$simres.cp7,
             simres_list$simres.cp8,
             simres_list$simres.cp9,
             simres_list$simres.cp10,
             simres_list$simres.cp11,
             simres_list$simres.cp12,
             simres_list$simres.cp13,
             simres_list$simres.cp14,
             simres_list$simres.cp15,
             simres_list$simres.cp16,
             simres_list$simres.cp17,
             simres_list$simres.cp18,
             simres_list$simres.cp19,
             simres_list$simres.cp20,
             simres_list$simres.cp21,
             simres_list$simres.cp22,
             simres_list$simres.cp23,
             simres_list$simres.cp24,
             simres_list$simres.cp25,
             simres_list$simres.cp26,
             simres_list$simres.cp27,
             simres_list$simres.cp28,
             simres_list$simres.cp29,
             simres_list$simres.cp30),
  multiplier = factor(rep(1:30, each = length(simres_list$simres.cp1)))
)

categ.palette <- viridis::viridis(30)

ggplot(data = df, aes(x = onsets, colour = multiplier)) + theme_gar +
  geom_freqpoly(binwidth = 2, fill = "white", na.rm = TRUE) +
  geom_vline(xintercept = true_onset, linetype = "dashed") +
  scale_colour_manual(values = categ.palette) +
  theme(legend.position = c(.8, .8)) +
  labs(x = "Onsets in ms", y = "Count", colour = "Penalty Multiplier")
```

To allow for a fair comparison with the Bayesian algorithm from the mcp package, a 100-200ms time window from which change points can be sampled was thought to be appropriate. The Bayesian approach contains a prior allowing to only detect change points in the 100-200ms segment of the data, meaning that the method will never be able to underestimate change point locations outside the 100-200ms time window. Therefore, to allow for a fair comparison with mcp, the PELT algorithm was limited in that only change point estimates within 100-200ms time are sampled.

The results of the simulation ran with the limits imposed on the PELT algorithm, shown in figure below, indicate that the majority of underestimated change points still occur when lower penalty multiplier values are used (as indicated by the blue and purple shaded lines). Multipliers 5, 10, 15, 20, 25 and 30 were isolated and compared below, where it can be seen that lower multiplier values produce more negatively biased. 

Ultimately, it was decided that the there will not be a sampling window limit for the main simulations.

```{r warning=FALSE, fig.height=10, fig.width=10}

load(file = "./data/simres_list_penalty_comparison_100ms_window.RData")

df <- tibble(
  onsets = c(simres_list$simres.cp1,
             simres_list$simres.cp2,
             simres_list$simres.cp3,
             simres_list$simres.cp4,
             simres_list$simres.cp5,
             simres_list$simres.cp6,
             simres_list$simres.cp7,
             simres_list$simres.cp8,
             simres_list$simres.cp9,
             simres_list$simres.cp10,
             simres_list$simres.cp11,
             simres_list$simres.cp12,
             simres_list$simres.cp13,
             simres_list$simres.cp14,
             simres_list$simres.cp15,
             simres_list$simres.cp16,
             simres_list$simres.cp17,
             simres_list$simres.cp18,
             simres_list$simres.cp19,
             simres_list$simres.cp20,
             simres_list$simres.cp21,
             simres_list$simres.cp22,
             simres_list$simres.cp23,
             simres_list$simres.cp24,
             simres_list$simres.cp25,
             simres_list$simres.cp26,
             simres_list$simres.cp27,
             simres_list$simres.cp28,
             simres_list$simres.cp29,
             simres_list$simres.cp30),
  multiplier = factor(rep(1:30, each = length(simres_list$simres.cp1)))
)

# Colour palette for six categories
categ.palette <- viridis::viridis(30)


ggplot(data = df, aes(x = onsets, colour = multiplier)) + theme_gar +
  geom_freqpoly(binwidth = 2, fill = "white", na.rm = TRUE) +
  geom_vline(xintercept = true_onset, linetype = "dashed") +
  scale_colour_manual(values = categ.palette) +
  theme(legend.position = c(.8, .8)) +
  xlim(0, 500) +
  labs(x = "Onsets in ms", y = "Count", colour = "Penalty Multiplier")
```

```{r warning=FALSE, fig.height=6, fig.width=8}


df_filter <- df |> 
  filter(multiplier %in% c(5,10,15,20,25,30))

categ.palette <- viridis::viridis(n_distinct(df_filter$multiplier))

ggplot(data = df_filter, aes(x = onsets, colour = multiplier)) + theme_gar +
  geom_freqpoly(binwidth = 2, fill = "white", na.rm = TRUE) +
  geom_vline(xintercept = true_onset, linetype = "dashed") +
  scale_colour_manual(values = categ.palette) +
  theme(legend.position = c(.8, .8)) +
  xlim(50, 300)
  labs(x = "Onsets in ms", y = "Count", colour = "Penalty Multiplier")
```